\section{Tensor Flow}
Let's follow the computation of our program to recognise the 10 digits, our input matrix is $(1 \times 784)$, our weights matrix W $(784 \times 10)$ our bias matrix $(10 \times 1)$.\\
Matrix representation of neural networks:
\[ 
     L = XW + B
\]L = logits. These are the steps:\[ 
    Pr(A(x)) = \sigma(xW +b) 
\]
\[ 
    L(x) = -log(Pr(A(x)=a)) \quad \text{cross entrpy loss function}
\]
\[ 
    \nabla_l X(\Phi) = \left( \frac{\partial X(\Phi)}{\partial l_1}, \ldots, \frac{\partial X(\Phi)}{\partial l_m} \right) 
\]
\[ 
    \Delta W = -\mathcal{L}X^T\nabla_l X(\Phi) 
\]
Important we want to manipulate input in batches. Now the input matrix is now (m x 784), m batch size. When adding bias you might have to adjust the size of the matrix to work with the m rows of the batch input matrix. Tensor flow is the programming language and Python is the environment. Tensorflow plays with tensors (often typed). Vectors in Tensorflow only have one dimension! In the environment we set-up our nn-model by defining parameters and an architecture. We go trhough 3 stages: 
\begin{itemize}
    \item Create the tensors
    \item Turn them into a variable
    \item Initialize them
\end{itemize}
A tensorflow program:
\begin{itemize}
    \item Load data
    \item Set-up the model (batch-size)
    \item Learn the variables (parameters)
\end{itemize}
Along the way of training the model we must keep in mind to compute the accuracy.\\ Rule of thumb: the smaller the batch size the smaller the learning rate.\\
\subsection{Multilayered NNs}
Our model can work with multiple layered NNs but we cannot obtain any improvement if we don't include some non-linearity in the sense of an activation function on the neurons. We can clearly see it if we see that a one layer feed-forward NN is simply computing $y = XW$. If we add another layer of linear units U which feeds into a layer V:
\begin{align*}
    y &= (xU)V\\
    &= x(UV)
\end{align*}
Whatever capabilities captured in the two layers situation can be compacted in the single matrix $W = UV$.
Examples of activation functions are ReLU (rectified linear unity) and the Sigmoid. They are placed between the output of one layer and the input of the next one.\\
A key problem that we quickly encounter is the vanishing gradient that the ReLU solves better than the Sigmoid. Another improvement over the ReLU is the Leaky ReLU.
\[ 
    Pr(A(x)) = \underset{soft-max}{\sigma} (\underset{ReLU}{\rho}(xU+\underset{first-layer bias}{b_u})V+\underset{second-layer bias}{b_v}) 
\]

