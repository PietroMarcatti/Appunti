\section{Word Embeddings}
 When introducing the concept of word embeddings we can say that our aim is to build a language model in terms of a probability distribution over strings (first words and then sentences). These elements will constitute the features of our model. The characterization through the use of a probability distribution is a key aspect.\\
 A sentence is a sequence of words and for example we want to calculate the probability of the sentence
 \[ 
    \text{We live in a small world} 
 \]
 we are interested in
 \[ 
    P(We, live, in, a, small, world) = P(world \mid we, live, in , a, small) P(we, live, in , a ,small) 
 \]This is theoretically correct but the probabilities get so small that it is not reasonable to use them.\\
 One question we can ask ourselves how do we recognize the words from eachother? The answer is tokenization, we define a corpus and cap the vocabulary (example 10000 words) and its size is going to be one of our parameters. Having a fixed vocabulary means that we must account for words that are not in our vocabulary, for them we must include a unknown token.\\The important issue, already mentioned, is that the longer the prefix sentence gets, the more miningless the probabilities get. For this matter we reduce the span of the probability computation. We start by considering the smallest window possible: 2
 \subsection{Bigram Model}
 The simple idea is: given a word we want to give the most probable next word. Three possible approaches
 \begin{description}
    \item[Super-trivial]: every word is equally probable
    \item[Trivial]: count the occurences
    \item[Bigram]: estimate "two words" reliably and in a generalizable way   
 \end{description}
 To calculate this estimate we use deep learning. We want to give the deep network a word and expect as output a reasonable probability distribution over possible next words. It is obvious that we need, somehow, to turn words into floating point numbers that deep learning can manipulate. The vectors representing the words in floating point are called word embeddings.\\
 The number of features associated with each embedding is an hyperparameter of the network.\\
 Insert description of the network\\
 Notice the difference between this and a classifier, the output modifies the encoding. A critical point is that E, the embedding, is a parameter of the model and they are initialized randomly and learned using a stochastic gradient descent optimization. This process does actually converge to a solution and this solution has the property to map words that have similar meanings to similar embedding vectors. A question then emerges: how do we measure closeness? We use scalar prouct and in particular cosine similarity 
 \[ 
    cos(x,y) = \frac{x\cdot y}{(\sqrt{(\sum_{i=1}^{n}{x_i^2})})(\sqrt{(\sum_{i=1}^{n}{y_i^2})})} 
 \]
 Given a word, then most probable next on is going to have the largest cosine similarity.
