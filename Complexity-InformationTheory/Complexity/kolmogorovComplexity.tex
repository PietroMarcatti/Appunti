\section{Complexity}
    \subsection{Kolmogorov}
        \subsubsection{History}
        We've talked about Shannon who worked in the US, studied at MIT and Princeton, collaborated with Bell Labs.
        We've talked about Fano who started at the Politecnico di Torino and then moved to MIT.
        Then we talked about Huffman who worked at Ohio State University, then founds the University of California Santa Cruz.\\
        Kolmogorov, unlike the other names we've seen so far, does not come from the west. In 1965 he was working at the University of Moscow\\
        \subsubsection{Idea}
        Consider a computational model and define the complexity of a string as the lenght of the shortest program (in that computational model)
        that can generate that string. We'll use Turing Machines: $m = (k, \Sigma,\delta, s)$
        \begin{description}
            \item[k]: finite set of states
            \item[$\Sigma$]: finite alphabet\quad  $\triangleright, \sqcup \in \Sigma$
            \item[$\delta$]: $K \times \Sigma \longrightarrow (K\cup\{yes, no, halt\}) \times  \Sigma \times \left\{\rightarrow, \leftarrow, -\right\}$
            \item[s]: $s \in K$ start state
        \end{description}
    \subsubsection{Church-Turing Thesis}
    All computational models are turing-equivalent.\newline
    Turing machines with k tapes and I/O
    $m=(K,\Sigma, \delta, s)$
    \begin{itemize}
        \item $\delta: \Sigma^k x K \rightarrow \Sigma^k x (K \cup {yes, no, halt}) x \left\{ \rightarrow, \leftarrow, - \right\}^k$
        \item The input tape (1st tape) cannot be modified but we can move backwards as much as possible
        \item The output tap (last tape) can only go forwards
    \end{itemize}
    \subsubsection{Universal Machine Theorem}
    $$\exists u, u \text{Universal Turing Machine} | u(bin(m), x) = m(x)$$
    How does the universal turing machine works:
    its tape is somethink like this: start $ \left[ 01\ldots 1010 \right] $ x
    To be able to go on with the computation if we were in the middle of it, we need to know the configuration of the machine.
    The configuration is stored in one of the k tapes. We also need the state we are on and the position of the first tape.
    The tape and the position can be expressed as a triple $(q, w, \varsigma)$
    \begin{itemize}
        \item q is the current character
        \item w is the string left of q
        \item $\varphi $is the string right of q
    \end{itemize}
    The Kolmogorov complexity of a string m is denoted by $K_u(x) = \underset{u(bin(m))=x}{min}|bin(m)|$
    \subsubsection{Observation}
    Unfortunately this notion is not computable. $$
    \nexists\; A\quad \text{Turing Machine} \quad|\quad A(x) = K_u(x)$$
    \subsubsection{Conditional Kolmogorov Complexity}
    The Conditional Kolmogorov Complexity of x given y is:
    $$ K_u(x|y) = \underset{u(bin(m),y)=x}{min} |bin(m)| $$
    This means that, obviously, if we provied some more information $K_u(x|y) \leq K_u(x)$. One of the most common information given is $|x|$.\\
    
    \begin{theorem}[Universality of Kolmogorov Complexity]
        If $\mathcal{U}$ is a universal computer (Turing Machine), for any other computer $\mathcal{A}$ there exists a constant $c_\mathcal{A}$ such that \[ 
            K_\mathcal{U}(x) \leq K_\mathcal{A} + c_\mathcal{A} 
        \]
        for all strings $x \in {0,1}^*$, and the constant $c_\mathcal{A}$ does not depend on x.
        \begin{proof}
            Assume that we have a program $p_\mathcal{A}$ for computer $\mathcal{A}$ to print x. Thus, $\mathcal{A}(p_\mathcal{A}) = x$. We can precede this program by a simulation program $\mathcal{s_A}$ which tells the computer $\mathcal{U}$ how to simulate computer $\mathcal{A}$. Computer $\mathcal{U}$ will then interpret the instructions in the program for $\mathcal{A}$, perform the corresponding calculations and print out x. The program for $\mathcal{U}$ is $p = s_\mathcal{A}p_\mathcal{A}$ and its length is
            \[ 
                l(p) = l(s_\mathcal{A}) + l(p_\mathcal{A}) = c_\mathcal{A}+l(p_\mathcal{A})
            \]
            where $c_\mathcal{A}$ is the lenght of the simulation program. Hence, for all strings x
            \[ 
                K_\mathcal{A}(x) = \min_{p:\mathcal{U}(p)=x}{l(p)} \leq \min_{p:\mathcal{A}(p)=x}{l(p)+c_\mathcal{A}} = K_\mathcal{A}(x) +c_\mathcal{A}
            \]
        \end{proof}
    \end{theorem}
    
    The crucial point is that the lenght of this simulation program is independent of the length of x, the string to be compressed. For sufficiently long x, the length of this simulation program can be neglected, and we can discuss Kolmogorov complexity without talking about the constants.\\
    \begin{theorem}[Upper bound for conditional kolmogorov complexity]
        \[ 
            K(x\mid l(x))\leq l(x) +c 
        \]
        \begin{proof}
            Let's define a program for printing x as the program that says:
            \[ 
                \text{Print the following l-bit sequence:}\; x_{1}x_{2}\ldots,x_{l(x)}
            \]
            Note that no bits are required to describe l since l is given. The program is self-delimiting because l(x) is provided and the end of the program is thus clarly defined. The length of this program is $ l(x) +c $ 
        \end{proof}
    \end{theorem}
    An apparently stronger conclusion is found in the book "An introduction to Kolmogorov Complexity and its applications" by M. Li and P. Vitanyi, even though this relies on an important detail. In fact, in this case, the alphabet for the machine is ${0,1,\sqcup}$. If we allow ourselves to make this assumption we can write the following.
    \begin{theorem}[Upper bound for kolmogorov complexity - Li-Vitanyi]
        \[ 
            K(x)\leq l(x) +c 
        \]
        \begin{proof}
            To obtain this stronger looking upper bound for the Kolmogorov complexity, Li and Vitanyi made an assumption about the tape of the machine and its alphabet. In fact, they consider the input tape of the machine as containing the starting symbol $\triangleright$ followed by all the binary digits of x and after it an infinite sequence of blank spaces $\sqcup$. This way they implicitly give the length of x because the machine won't find a $\sqcup$ under its cursor untile it hasn't traversed all x, gaining the information about it's length.
        \end{proof}
    \end{theorem}
        
    \begin{theorem}[Upper bound for kolmogorov complexity - Cover]
        \[ 
            K(x)\leq K(x\mid l(x)) + 2\log{l(x)}+c 
        \]
        \begin{proof}
            If the computer does not know $l(x)$ we must have some way of informing the computer when it has to come to the end of the string of bits that describes the sequence. We describe a simple but inefficient method that uses a sequence 01 as a "comma".\\
            Suppose that $l(x) = n$. To describe $l(x)$, repeat every bit of the binary expansion of n twice; then end the description with a 01 so that the computer knows that it has come to the end of the description of n.\\For example, the number 5 (binary 101) will be described as 11001101. This description requires $ 2 \lceil \log{n} \rceil +2 $ bits. Thus, the inclusion of the binary representation of $l(x)$ does not add more than $2\log{l(x)}+c$ bits to the length of the program, and we have the bound in the theorem.
        \end{proof}
    \end{theorem}

    \begin{lemma}
        The set of strings having a Kolmogorov complexity $<a$ has less than $2^a$ elements.
        \begin{proof}
            If x has $K(x) < a$ thene there exists the program $m$ that produces x and $|m|<a$. Each $m$ produces at most 1 string. The question then shifts to how many programs of length $<a$ are there at most?
            \[ 
                 \sum_{k=0}^{a-1}{2^k} = 2^a -1
            \]
            Then there are at most $2^a-1$ possible strings with Kolmogorov complexity $<a$.
        \end{proof}
    \end{lemma}

    \begin{theorem}
        There exists a string x such that
        \[ 
            K(x) \geq |x| 
        \]
        \begin{proof}
            Let $|x|=a$, there are $2^a$ strings of length a. At most $2^a-1$ of these have Kolmogorov complexity $\leq a-1$. Then there exists at least one string of length a having Kolmogorov complexity $\geq a$
        \end{proof}
    \end{theorem}
    
    \subsubsection{Kolmogorov Encoding}
    Consider the Kolmogorov code defined as follows
    \[ 
        \varphi_K: A* \longrightarrow \left\{ m | m \text{Turing Machine} \right\} 
    \]
    $ \varphi_K(x) = bin(m)$ such that $m$ is the shortest machine such that \[ 
         m(\varepsilon) = x
    \] 
    $\varphi_K(x)$ is a U.D. code and a universal machine U decodes $\varphi_K$
    \[ 
        |\varphi_K(x)| = K(x) = K_U(x) 
    \]
    Focusing on the language of the strings in $SA^n$, the average length of $\varphi_K$ over $A^n$ is
    \[ 
        EL_n(\varphi_K) = \sum_{x\in A^n}{p(x)\cdot K(x)} = E_n(K)
    \]
    $E_n(K)$ is the average of the Kolmogorov complexity over the strings of length n.
    From Shannon Theorem we get
    \[ 
        EL_n(\varphi_K) \geq \mathcal{H}(P^n) = n\cdot \mathcal{H}_D(P) 
    \]which leads us to conlcude
    \[ 
        E_n(K)\geq n\cdot \mathcal{H}(P) 
    \]
    \[ 
        \frac{E(K) }{n} \geq \mathcal{H}_3(P)
    \]

    To set a lower bound we reason as follows: any U.D. code $\varphi$ can be seen as a set of machines for producing strings. The couple \{Decoder for $\varphi$ ; $\varphi(x)$ \} is an algorithm that produces x as output.
    \[ 
        \forall x\quad K(x) \leq |Decoder(\varphi)|+|\varphi(x)| 
    \]

    \begin{align*}
        E_n(K(x)) &\leq E(|Decoder \varphi| +|\varphi(x)|)
        E_n(K(x)) &\leq |Decoder \varphi|+ \sum_{x\in A^n}{p(x)|\varphi(x)|}\\
        & \leq |Decoder \varphi| + EL_n(\varphi)\\
    \end{align*}
    If applied to $\varphi$ as Shannon Code over strings of length n:
    \begin{align*}
        E_n(K(x)) &\leq |Decoder \varphi| + \mathcal{H}(P^n)+1\\
        E_n(K(x)) &\leq K(P) + n\mathcal{H}(P)+1\\
        \frac{E_n(K(x))}{n} &\leq \mathcal{H}(P) + \frac{K(P)+1}{n}
    \end{align*}
    \[ 
        \mathcal{H}(P)\leq \frac{E_n(K(x))}{n} \leq \mathcal{H}(P) + \frac{K(P)+1}{n} 
    \]
    

